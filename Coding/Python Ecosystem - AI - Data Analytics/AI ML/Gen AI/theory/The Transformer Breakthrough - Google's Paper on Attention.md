# Attention is all you need (By Google)
[View paper](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

> Refer the transformer architecture in this paper for this guide

---
# Read these
[[Vector Embeddings]]
[[Role of Positional Encodings in Tranformers and Self Attention Mechanism]]
[[Self Attention]]
[[Multi-Head Attention Mechanism]]
[[Feed Forward - Linear - Softmax in LLM Transformer]]

---