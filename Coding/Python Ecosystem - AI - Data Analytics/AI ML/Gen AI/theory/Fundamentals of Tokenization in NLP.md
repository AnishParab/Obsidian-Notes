# Tokens
The characters that humans use are mapped to some numbers is known as tokens.

---
# View how tokens are generated
[Click here](https://tiktokenizer.vercel.app/)

---
# Tokenization
- Converting the user input to set of numbers understandable by LLM.
- LLM takes the tokens as input and generates a new token.

---
# Detokenization
- Converting the number back to english, this is returned to the user.

---
