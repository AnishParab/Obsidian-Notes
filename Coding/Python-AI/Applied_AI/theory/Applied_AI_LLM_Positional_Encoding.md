# Why Positional Encoding ?
- **Dog** ate **Cat**.
- **Cat** ate **Dog**
> The vector embedding for this will be same.

---
# Steps
### Tokenization
- Dog: 56
- ate: 74
- Cat: 89
### Vector Embeddings
- Gives Semantic meaning.
### Positional Encoding
- 56: 0
- 74: 1
- 89: 2
> Hence it defines how the meaning of the whole sentence can change if positions are misplaced.

---
