
![[llm_basics.excalidraw|500]]

---
# What is GPT?
- **GPT** stands for **Generative Pre-trained Transformer**.

---
# Breakdown
- **Generative**  
    The model generates new text token by token (it does not retrieve fixed answers).
- **Pre-trained**  
    It is first trained on large amounts of text data to learn general language patterns _before_ being adapted to specific tasks.
- **Transformer**  
    It uses the Transformer architecture (self-attentionâ€“based neural network).

---
# What GPT actually is
- GPT is a **type of Large Language Model (LLM)**
- It is trained to **predict the next token** given previous tokens
- From this simple objective, it learns:
    - Grammar
    - Facts
    - Reasoning patterns
    - Code structure

---
