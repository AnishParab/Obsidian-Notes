# Google's Paper on Transformer Architecture
- They used it for **Google Translate**.
[Attention is all you need](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)
- It tranformed for example, English **sequence** to Hindi **sequence**

---
# GPT
- GPT takes **input tokens** to predict the next **token**.

![[llm_transformer.excalidraw|500]]

---
> Hence this requires **high compute power**.

---
